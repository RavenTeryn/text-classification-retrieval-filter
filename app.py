import os
# --- 1. é…ç½®é•œåƒæº ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import streamlit as st
import time
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# --- 2. é¡µé¢è®¾ç½® (æ”¹ä¸ºä¾§è¾¹æ å¯¼èˆªé£æ ¼) ---
st.set_page_config(
    page_title="å¤šé¢†åŸŸçŸ¥è¯†åˆ†ç±»æ£€ç´¢ç³»ç»Ÿ",
    page_icon="ğŸ“‚",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 3. æ ¸å¿ƒé€»è¾‘ï¼šè‡ªåŠ¨åˆ†ç±»ä¸ç´¢å¼• ---
@st.cache_resource
def initialize_system():
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    
    # è¯»å– data ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
    loader = DirectoryLoader('data/', glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    raw_docs = loader.load()
    
    if not raw_docs:
        return None, None, {}

    # --- å…³é”®ä¿®æ”¹ï¼šè‡ªåŠ¨æ‰“æ ‡ç­¾ (Text Classification æ¨¡æ‹Ÿ) ---
    categorized_docs = []
    categories = set()
    
    # 1. å®šä¹‰æ–°çš„å…³é”®è¯åˆ—è¡¨ (å¯¹åº”ç”Ÿæˆæ•°æ®çš„ä¸‰ä¸ªç±»åˆ«)
    # AI ä¿æŒä¸å˜
    ai_keywords = ['learning', 'neural', 'intelligence', 'gpt', 'python', 'data', 'cloud']
    
    # FinTech (é‡‘èç§‘æŠ€) - æ›¿ä»£åŸæ¥çš„ geo
    fintech_keywords = ['blockchain', 'bitcoin', 'payment', 'finance', 'wallet', 'economy', 'bank']
    
    # Humanities (äººæ–‡å¸¸è¯†) - æ›¿ä»£åŸæ¥çš„ sci
    humanities_keywords = ['history', 'culture', 'art', 'philosophy', 'literature', 'civilization', 'museum']
    
    for doc in raw_docs:
        filename = doc.metadata['source'].lower()
        content = doc.page_content.lower()
        
        # é»˜è®¤åˆ†ç±»
        category = "å…¶ä»–èµ„è®¯ (General)"
        
        # æ ¹æ®æ–‡ä»¶åæˆ–å†…å®¹åˆ¤æ–­åˆ†ç±»
        if any(k in filename or k in content for k in ai_keywords):
            category = "ğŸ¤– AIä¸å‰æ²¿æŠ€æœ¯"
        elif any(k in filename or k in content for k in fintech_keywords):
            category = "ğŸ’° é‡‘èç§‘æŠ€è§‚å¯Ÿ"
        elif any(k in filename or k in content for k in humanities_keywords):
            category = "ğŸ“š äººæ–‡å†å²å¸¸è¯†"
            
        # å°†åˆ†ç±»å†™å…¥ metadata
        doc.metadata['category'] = category
        categorized_docs.append(doc)
        categories.add(category)

    # åˆ‡åˆ†æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = text_splitter.split_documents(categorized_docs)
    
    # å»ºç«‹å‘é‡ç´¢å¼•
    vector_db = FAISS.from_documents(splits, embeddings)
    
    return vector_db, raw_docs, list(categories)

# --- 4. åˆå§‹åŒ– ---
with st.spinner("æ­£åœ¨åŠ è½½åˆ†ç±»æ¨¡å‹ä¸çŸ¥è¯†åº“..."):
    vector_db, raw_docs, category_list = initialize_system()

# --- 5. UI å¸ƒå±€ï¼šå·¦ä¾§ç­›é€‰ï¼Œå³ä¾§æ£€ç´¢ ---
# Topic 2 è¦æ±‚ï¼šClassification labels as filters 

with st.sidebar:
    st.header("ğŸ“‚ é¢†åŸŸå¯¼èˆª")
    st.markdown("è¯·é€‰æ‹©è¦æ£€ç´¢çš„çŸ¥è¯†é¢†åŸŸï¼š")
    
    # æ·»åŠ â€œå…¨éƒ¨â€é€‰é¡¹
    selected_category = st.radio(
        "é€‰æ‹©åˆ†ç±» (Topic Filter):",
        ["ğŸŒ å…¨éƒ¨é¢†åŸŸ (All Topics)"] + sorted(list(category_list))
    )
    
    st.markdown("---")
    st.info(f"ğŸ“š å½“å‰åº“ä¸­æ–‡æ¡£æ€»æ•°: {len(raw_docs)}")
    if selected_category != "ğŸŒ å…¨éƒ¨é¢†åŸŸ (All Topics)":
        # ç»Ÿè®¡å½“å‰åˆ†ç±»ä¸‹çš„æ–‡æ¡£æ•°
        count = sum(1 for d in raw_docs if d.metadata.get('category') == selected_category)
        st.success(f"å½“å‰åˆ†ç±»åŒ…å«æ–‡æ¡£: {count} ç¯‡")

# ä¸»ç•Œé¢
st.title("ğŸ“‘ Topic-Filtered Retrieval System")
st.caption("åŸºäºæ–‡æœ¬åˆ†ç±»çš„å®šå‘æ£€ç´¢ç³»ç»Ÿ | Topic 2 Implementation")

# æœç´¢åŒº
query = st.text_input("åœ¨è¯¥é¢†åŸŸå†…æœç´¢å…³é”®è¯...", placeholder="è¾“å…¥æŸ¥è¯¢å†…å®¹...")
search_btn = st.button("ğŸ” æ£€ç´¢æ–‡æ¡£", type="primary")

if (query or search_btn) and vector_db:
    # --- æ£€ç´¢é€»è¾‘ ---
    # 1. å…ˆè¿›è¡Œå‘é‡æ£€ç´¢ (å¬å› Top 10ï¼Œå¤šæ‹¿ä¸€ç‚¹æ–¹ä¾¿åé¢è¿‡æ»¤)
    results = vector_db.similarity_search(query, k=15)
    
    # 2. åç½®è¿‡æ»¤ (Post-filtering)ï¼šåªä¿ç•™ç”¨æˆ·é€‰ä¸­åˆ†ç±»çš„ç»“æœ
    if selected_category != "ğŸŒ å…¨éƒ¨é¢†åŸŸ (All Topics)":
        filtered_results = [doc for doc in results if doc.metadata.get('category') == selected_category]
    else:
        filtered_results = results

    # å–å‰ 4 ä¸ªå±•ç¤º
    final_results = filtered_results[:4]

    st.markdown(f"### ğŸ” '{selected_category}' é¢†åŸŸä¸‹çš„æ£€ç´¢ç»“æœ")
    
    if not final_results:
        st.warning(f"åœ¨ '{selected_category}' åˆ†ç±»ä¸‹æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
    else:
        for doc in final_results:
            cat_tag = doc.metadata.get('category')
            source_name = doc.metadata['source'].split('/')[-1]
            
            # ä½¿ç”¨ Streamlit çš„ expander æ ·å¼å±•ç¤ºï¼Œçœ‹èµ·æ¥åƒæ–‡ä»¶åˆ—è¡¨
            with st.expander(f"ğŸ“„ {source_name}  [{cat_tag}]", expanded=True):
                st.markdown(f"**...{doc.page_content}...**")
                st.caption(f"æ¥æº: {doc.metadata['source']}")

elif not vector_db:
    st.error("æœªæ‰¾åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ data æ–‡ä»¶å¤¹ã€‚")