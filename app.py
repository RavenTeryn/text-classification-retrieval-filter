import os
# --- 1. é…ç½®é•œåƒæº ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import streamlit as st
import time
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# --- 2. é¡µé¢è®¾ç½® (å» Emojiï¼Œæ”¹ç”¨ä¸“ä¸šå›¾æ ‡) ---
st.set_page_config(
    page_title="InfoStream - ä¸“ä¸šèµ„è®¯å½’æ¡£ç³»ç»Ÿ",
    page_icon="ğŸ“‘",  # ä»…ä¿ç•™æ ‡é¢˜æ ä¸€ä¸ªå›¾æ ‡
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 3. CSS æ·±åº¦å®šåˆ¶ (å»å¡ç‰‡åŒ–ï¼Œèµ°ä¸“ä¸šæ–‡æ¡£é£) ---
st.markdown("""
<style>
    /* å…¨å±€å­—ä½“ä¸èƒŒæ™¯ - æ›´åŠ å†·æ·¡ä¸¥è°¨ */
    .stApp {
        background-color: #FAFAFA;
    }
    
    /* ä¾§è¾¹æ æ ·å¼é‡ç½® */
    [data-testid="stSidebar"] {
        background-color: #F0F2F6;
        border-right: 1px solid #E0E0E0;
    }
    
    /* æ ‡é¢˜æ ·å¼ - æ·±è‰²è¡¬çº¿ä½“ */
    h1, h2, h3 {
        color: #262730;
        font-family: 'Helvetica Neue', sans-serif;
    }
    
    /* æœç´¢ç»“æœåˆ—è¡¨é¡¹æ ·å¼ (æ›¿ä»£ä¹‹å‰çš„ Card) */
    .result-item {
        padding: 15px 0;
        border-bottom: 1px solid #E6E6E6;
    }
    .result-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1A73E8; /* Google Link Blue */
        margin-bottom: 5px;
    }
    .result-meta {
        font-size: 0.85rem;
        color: #5F6368;
        font-family: monospace;
        margin-bottom: 8px;
    }
    .result-snippet {
        font-size: 0.95rem;
        color: #3C4043;
        line-height: 1.5;
    }
    
    /* éšè— Streamlit é»˜è®¤çš„æŒ‰é’®è¾¹æ¡†ï¼Œè®©ç•Œé¢æ›´å¹²å‡€ */
    div.stButton > button {
        border-radius: 4px;
        background-color: #008080; /* Teal Color */
        color: white;
        border: none;
    }
    div.stButton > button:hover {
        background-color: #006666;
    }
</style>
""", unsafe_allow_html=True)

# --- 4. æ ¸å¿ƒé€»è¾‘ï¼šè‡ªåŠ¨åˆ†ç±»ä¸ç´¢å¼• ---
@st.cache_resource
def initialize_system():
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    
    # è¯»å– data ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
    loader = DirectoryLoader('docs/', glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    raw_docs = loader.load()
    
    if not raw_docs:
        return None, None, []

    # --- å…³é”®ä¿®æ”¹ï¼šè‡ªåŠ¨æ‰“æ ‡ç­¾ ---
    categorized_docs = []
    
    # å…³é”®è¯å®šä¹‰ (ä¿æŒä½ çš„æ–°åˆ†ç±»)
    ai_keywords = ['learning', 'neural', 'intelligence', 'gpt', 'python', 'data', 'cloud']
    fintech_keywords = ['blockchain', 'bitcoin', 'payment', 'finance', 'wallet', 'economy', 'bank']
    humanities_keywords = ['history', 'culture', 'art', 'philosophy', 'literature', 'civilization', 'museum']
    
    for doc in raw_docs:
        filename = doc.metadata['source'].lower()
        content = doc.page_content.lower()
        
        # é»˜è®¤åˆ†ç±» (æ—  Emoji)
        category = "General / Uncategorized"
        
        # æ ¹æ®æ–‡ä»¶åæˆ–å†…å®¹åˆ¤æ–­åˆ†ç±» (ç§»é™¤ Emoji)
        if any(k in filename or k in content for k in ai_keywords):
            category = "AI & Technology"
        elif any(k in filename or k in content for k in fintech_keywords):
            category = "FinTech & Economy"
        elif any(k in filename or k in content for k in humanities_keywords):
            category = "Humanities & History"
            
        doc.metadata['category'] = category
        categorized_docs.append(doc)

    # å¼ºåˆ¶å®šä¹‰åˆ†ç±»åˆ—è¡¨é¡ºåº (è§£å†³åˆ†ç±»æ˜¾ç¤ºä¸å…¨çš„é—®é¢˜)
    # å³ä½¿æ–‡ä»¶å¤¹é‡Œæ²¡æœ‰æ–‡ä»¶ï¼Œè¿™äº›é€‰é¡¹ä¹Ÿä¼šæ˜¾ç¤ºï¼Œä¿è¯ UI ç»“æ„å®Œæ•´
    fixed_categories = ["AI & Technology", "FinTech & Economy", "Humanities & History", "General / Uncategorized"]

    # åˆ‡åˆ†æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = text_splitter.split_documents(categorized_docs)
    
    # å»ºç«‹å‘é‡ç´¢å¼•
    vector_db = FAISS.from_documents(splits, embeddings)
    
    return vector_db, raw_docs, fixed_categories

# --- 5. åˆå§‹åŒ– ---
with st.spinner("Initializing Archives..."):
    vector_db, raw_docs, category_list = initialize_system()

# --- 6. ä¾§è¾¹æ ï¼šæ§åˆ¶é¢æ¿é£æ ¼ ---
with st.sidebar:
    st.markdown("### ğŸ—‚ï¸ Document Navigator")
    
    # ä½¿ç”¨ Radio ç»„ä»¶ä½†æ ·å¼æ›´ç®€æ´
    selected_category = st.radio(
        "Select Category:",
        ["ALL ARCHIVES"] + category_list
    )
    
    st.markdown("---")
    
    # ä»ªè¡¨ç›˜å¼çš„æ•°æ®å±•ç¤º
    col1, col2 = st.columns(2)
    with col1:
        st.metric(label="Total Docs", value=len(raw_docs))
    with col2:
        if selected_category != "ALL ARCHIVES":
            count = sum(1 for d in raw_docs if d.metadata.get('category') == selected_category)
            st.metric(label="Current", value=count)
        else:
            st.metric(label="Current", value="All")

    st.markdown("---")
    st.caption("System v2.0 | Topic 2 Classification Build")

# --- 7. ä¸»ç•Œé¢ï¼šæœç´¢å¼•æ“é£æ ¼ ---

st.markdown("## ğŸ” Information Retrieval System")
st.markdown("Type keywords to search across the categorized database.")

# æœç´¢æ å¸ƒå±€ï¼šæ›´åƒ Google
search_col1, search_col2 = st.columns([5, 1], vertical_alignment="bottom")

with search_col1:
    query = st.text_input("Search Query", placeholder="e.g., impact of blockchain", label_visibility="collapsed")
with search_col2:
    search_btn = st.button("Search", use_container_width=True)

st.markdown("---")

# --- 8. æ£€ç´¢ä¸ç»“æœå±•ç¤º (åˆ—è¡¨å¼ï¼Œéå¡ç‰‡å¼) ---
if (query or search_btn) and vector_db:
    start_time = time.time()
    
    # 1. å®½æ³›å¬å›
    results = vector_db.similarity_search(query, k=20)
    
    # 2. ä¸¥æ ¼è¿‡æ»¤
    if selected_category != "ALL ARCHIVES":
        filtered_results = [doc for doc in results if doc.metadata.get('category') == selected_category]
    else:
        filtered_results = results

    # å– Top 5
    final_results = filtered_results[:5]

    # æ˜¾ç¤ºç»“æœå¤´
    if not final_results:
        st.warning(f"No results found in category: {selected_category}")
    else:
        st.markdown(f"**Found {len(final_results)} relevant documents** ({time.time() - start_time:.4f}s)")
        
        for doc in final_results:
            cat_tag = doc.metadata.get('category')
            file_name = doc.metadata['source'].split('/')[-1]
            
            # ä½¿ç”¨ HTML æ„å»ºâ€œè°·æ­Œå­¦æœ¯â€é£æ ¼çš„åˆ—è¡¨
            st.markdown(f"""
            <div class="result-item">
                <div class="result-title">ğŸ“„ {file_name}</div>
                <div class="result-meta">
                    <span style="background-color: #E0F2F1; color: #00695C; padding: 2px 6px; border-radius: 4px;">{cat_tag}</span>
                    &nbsp; â€¢ &nbsp; Relevance Match
                </div>
                <div class="result-snippet">
                    ...{doc.page_content}...
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            # ä½¿ç”¨åŸç”Ÿ expander æŸ¥çœ‹å…¨æ–‡ (æŠ˜å èµ·æ¥ä¿æŒå¹²å‡€)
            with st.expander("View Full Context"):
                st.text(doc.page_content)

elif not vector_db:
    st.error("Database Error: Please check data directory.")
elif not query:
    st.info("Awaiting input... Select a category from the sidebar to browse.")